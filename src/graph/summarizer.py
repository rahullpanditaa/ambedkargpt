"""
Community-level summarization for Global Graph RAG (SemRAG).

This module generates concise, factual summaries for each entity
community detected in the knowledge graph. These summaries act as
community reports used during Global Graph RAG retrieval
(Equation 5 in the SemRAG paper).

Summaries are generated by selecting representative chunks per
community and prompting a local LLM to synthesize their themes.
"""
import json
import ollama
from pathlib import Path
from src.utils.constants import (
    PROCESSED_DATA_DIR_PATH, 
    ENTITY_COMMUNITY_PATH, 
    CHUNK_ENTITIES_PATH,
    CHUNKS_OUTPUT_PATH,
    COMMUNITY_SUMMARIES_PATH,
    PROMPT,
    MIN_CHUNKS_PER_COMMUNITY,
    MIN_ENTITIES_PER_COMMUNITY,
    TOKENS_PER_COMMUNITY
)

class CommunitySummarizer:
    """
    Generates summaries for entity communities using representative chunks.

    This class:
    - Groups entities by detected community
    - Identifies chunks associated with each community via entity overlap
    - Selects representative chunks using frequency-based scoring
    - Uses a local LLM to generate concise community summaries

    The resulting summaries are used as high-level semantic signals
    during Global Graph RAG retrieval.
    """
    def __init__(self):
        """
        Initialize the community summarizer.

        Loads entity-to-community mappings and builds a reverse
        community-to-entity index. Communities with too few entities
        are filtered out to reduce noise.
        """
        PROCESSED_DATA_DIR_PATH.mkdir(parents=True, exist_ok=True)
        with open(ENTITY_COMMUNITY_PATH, "r") as f:
            entity_communities = json.load(f)
        self.entity_to_comm_id_map: dict = entity_communities["entity_communities"]
        self.comm_id_to_entities_map: dict = {}

        for entity, comm_id in self.entity_to_comm_id_map.items():
            if comm_id not in self.comm_id_to_entities_map.keys():
                self.comm_id_to_entities_map[comm_id] = []
            self.comm_id_to_entities_map[comm_id].append(entity)
        
        # filter such that only have communities with at least 5 entities
        filtered_comm_id_to_ents_map = {}
        for com_id, entities in self.comm_id_to_entities_map.items():
            if len(entities) >= MIN_ENTITIES_PER_COMMUNITY:
                filtered_comm_id_to_ents_map[com_id] = entities
        self.comm_id_to_entities_map = filtered_comm_id_to_ents_map


    def _collect_chunks_per_community(self):
        """
        Map communities to the set of chunk IDs associated with them.

        A chunk is considered part of a community if it contains
        at least one entity belonging to that community.

        Returns:
            dict: Mapping of community_id â†’ set of chunk_ids
        """
        ch_ents = load_json(CHUNK_ENTITIES_PATH)
        # list of dict where dict - chunk_id, entities: list[dict - text_norm, text_raw, label, count]
        chunk_entities = ch_ents["chunk_entities"]

        # community id -> set of chunk ids
        community_id_to_chunks = {}
        for chunk in chunk_entities:
            chunk_id = chunk["chunk_id"]
            current_chunk_in_communities = set()
            
            for entity in chunk["entities"]:
                if entity["text_norm"] in self.entity_to_comm_id_map:
                    chunk_community_id = self.entity_to_comm_id_map[entity["text_norm"]]
                    if chunk_community_id in self.comm_id_to_entities_map:
                        current_chunk_in_communities.add(chunk_community_id)
                    
            for community_id in current_chunk_in_communities:
                if community_id not in community_id_to_chunks:
                    community_id_to_chunks[community_id] = set()
                community_id_to_chunks[community_id].add(chunk_id)

        return community_id_to_chunks
    
    def _select_representative_chunks(self):
        """
        Select representative chunks for each community.

        Chunks are scored using the sum of entity frequencies
        within the chunk. Chunks are selected greedily until a
        token budget is reached.

        Returns:
            list[tuple]: Sorted list of (community_id, selected_chunks),
                         where selected_chunks is a list of dicts:
                {
                    "chunk_id": int,
                    "text": str
                }
        """
        # for each community - select top chunks until reach arbitrarily chosen token budget
        # return community id -> list of chunk texts

        # load required data
        community_id_to_chunks = self._collect_chunks_per_community()

        chunks_data = load_json(CHUNKS_OUTPUT_PATH)["chunks"]
        chunk_entities_data = load_json(CHUNK_ENTITIES_PATH)["chunk_entities"]

        # lookup maps - chunk id -> chunk text
        chunk_id_to_text = {
            ch["chunk_id"]: ch["text"] for ch in chunks_data
        }

        # chunk id -> entities
        chunk_id_to_entities = {
            ch["chunk_id"]: ch["entities"] for ch in chunk_entities_data
        }

        tokens_per_community = TOKENS_PER_COMMUNITY  # random
        selected_chunks_per_community = {}

        # process each community independently
        for community_id, chunk_ids in community_id_to_chunks.items():

            scored_chunks = []

            for chunk_id in chunk_ids:
                entities = chunk_id_to_entities.get(chunk_id, [])

                # sum of entity frequencies
                score = sum(ent["count"] for ent in entities)

                if score == 0:
                    continue

                scored_chunks.append({
                    "chunk_id": chunk_id,
                    "score": score,
                    "text": chunk_id_to_text[chunk_id]
                })

            # sort chunks by descending score
            scored_chunks.sort(key=lambda d: d["score"], reverse=True)

            selected_chunks = []
            used_tokens = 0

            for item in scored_chunks:
                chunk_text = item["text"]

                chunk_tokens = len(chunk_text.split())

                if used_tokens + chunk_tokens > tokens_per_community:
                    break

                selected_chunks.append({
                    "chunk_id": item["chunk_id"],
                    "text": chunk_text
                })

                used_tokens += chunk_tokens

            if not selected_chunks and scored_chunks:
                fallback = scored_chunks[0]
                selected_chunks.append({
                    "chunk_id": fallback["chunk_id"],
                    "text": fallback["text"]
                })

            selected_chunks_per_community[community_id] = selected_chunks

        # sort by chunk count
        sorted_results = sorted(selected_chunks_per_community.items(), key=lambda i: len(i[1]))
        return sorted_results
    
    def summarize_communities(self):
        """
        Generate LLM-based summaries for each community.

        For each community with sufficient representative chunks:
        - Constructs a constrained academic prompt
        - Feeds selected chunk texts to a local LLM
        - Stores the resulting summary

        Returns:
            list[dict]: List of community summaries with schema:
                {
                    "community_id": int,
                    "summary": str
                }
        """
        selected_chunks_per_community = self._select_representative_chunks()
        summaries = []
        # MAX_COMMUNITIES = 40
        for community_id, selected_chunks in selected_chunks_per_community:
            if not selected_chunks:
                continue
            # adding minimum filter to reduce number of communities
            if len(selected_chunks) < MIN_CHUNKS_PER_COMMUNITY:
                continue
            chunk_texts = []
            for chunk in selected_chunks:
                chunk_texts.append(chunk["text"])
            llm_input_chunks_text = "\n\n---\n\n".join(chunk_texts)
            prompt = PROMPT.replace("{CHUNKS_TEXT}", llm_input_chunks_text)
            response = ollama.generate(model="mistral", prompt=prompt)
            output = response.response
            if not output:
                continue
            summaries.append({
                "community_id": community_id,
                "summary": output
            })
            print(f"Community {community_id} summary:")
            print(f" - {output[:100]}...\n")

        with open(COMMUNITY_SUMMARIES_PATH, "w") as f:
            json.dump(summaries, f, indent=2)
        return summaries

def summarize_communities_command():
    cs = CommunitySummarizer()
    print("Generating summaries for all communities...")
    cs.summarize_communities()
    print("Summaries generated, stored to disk")
    


def load_json(file_path: Path):
    with open(file_path, "r") as f:
        result = json.load(f)
    return result